%!TEX root = ../index.tex

% 
% Architecture
% 

\section{Architecture}

In this section we describe our proposed architecture for the remaining implementation work. The software stack is composed by several subsystems that have one specific goal, exposing a well known API, this way the subsystems become interchangeable. These subsystems include:

\begin{itemize}
  \item \textbf{Communication service} - Responsible for routing messages between nodes in the DHT.
  \item \textbf{Service router} - Processes the messages that have as destiny the its node, the goal is to call the right service (storage, reputation mechanism, job execution, etc) accordingly. One other key aspect is the ability to attach new services during the runtime.
  \item \textbf{Storage service} - Stores any data that requires persistence in the network, such as job logs, reputation logs and file meta data and chunks.
  \item \textbf{Job coordination} - A subsystem responsible for coordinating jobs requested by the client, keeping state and assuring its completion/
  \item \textbf{Job execution} - Execution of jobs, gathering all the necessary assets (image processors, sound wave manipulators, etc) to complete the job/
  \item \textbf{Reputation Mechanism} - Validate user behavior and right to take different responsibilities in the network.
  \item \textbf{Client API and CLI} - In order to interact with the network, we offer an API and a CLI with Unix type instructions and familiar web cloud instructions that developers are familiar.
  \item \textbf{Rendezvous points} - The only centralized component in this architecture, its purpose is for the clients to have a way to connect to the overlay network. 
\end{itemize}

We can observe how this subsystems interconnect with each other by observing Figure~\ref{fig:softwarestack}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{img/softwarestack.jpg}
  \caption{Human's cloud Software Stack}
  \label{fig:softwarestack}
\end{figure}


\subsection{Network Architecture}

Network architecture can be seen on Figure \ref{fig:overallarchitecture}). Nodes (volunteer computers), are divided into two Chord DHTs with the purpose of separating the nodes with storage responsibility from the ones with only computing responsibility. The reason behind this decision is due to the high churn rate in a P2P network. Keeping the files in nodes have proven to be more trustworthy for staying longer in the network makes the system more robust by keeping the file replica level stable. This also reduces the message overhead that would require to keep the replica level in a more inconsistent environment. Nevertheless, the more volatile nodes are perfect for short computing operations, till they proven to be trustworthy to `ascend' in the network.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{img/overall.jpg}
  \caption{Human's cloud network architecture}
  \label{fig:overallarchitecture}
\end{figure}

Since browser cannot effectively have a static IP nor have a DNS record updated on demand pointing to themselves, we designed the API endpoints as the rendezvous between browsers and clients.

\subsection{Software architecture at the node level}

At the node level, we divide the application into two fundamental services and 3 pluggable components, with the possibility for expansion, thanks to Javascript dynamic runtime, we can find this structure in Figure~\ref{fig:hcnode}.

Starting with the communication layer, where the DHT logic is implemented, where the functional calls to propagate messages through the hash ring are implemented. 

Next, we have the Service Routing layer, this service is responsible to guide the message to the right component, this enables the architecture to be more modular, plugging in more components as it's needed, for example, when a node ascends and needs the storage component to fulfill his responsibility.

Last, we have the components, individual modules that do one thing and one thing well. Currently, we present the Storage module, responsible for holding the data, the Job Schedule, responsible to orchestrate jobs issued by the users, the assets needed to execute the jobs and finally the job executor, the module that will execute the jobs in a separate process using webworkers.

% Doug McIlroy, then head of the Bell Labs CSRC and contributor to Unix pipes,[1] summarised Unix philosophy as follows:[2]
% This is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.35\textwidth]{img/node.jpg}
  \caption{Human's Cloud Node}
  \label{fig:hcnode}
\end{figure}

\subsection{Storage}

Human's Cloud storage happens in what it's named, the ``Ascended node ring'', these nodes have an higher reliability, making the storage system more stable, without the need of constantly burning computer cycles to maintain the files replica level.

Data stored in nodes can be:
\begin{itemize}
  \item File metadata (name of the file, size, location of the chunks, chunks hash).
  \item File chunks
  \item Directories metadata - This way, hcls can be more efficient 
  \item Job information (state, issuer, workflow)
  \item Reputation log
\end{itemize}

We classify storage nodes into two types: 1) the `sKeeper', responsible for holding the metadata of the file and hashing each chunk to identify the `sHolder'; 2) `sHolder' nodes responsible to store the chunk into their system. This approach mitigates the possibility of having an highly unbalanced storage distribution, dividing each file in equal chunks across several nodes. As we can see in Figure~\ref{fig:chunking}, each chunk gets hashed more than one time with a different hash function, the purpose is to identify several Nodes that will be responsible to store a replica of the chunk. Also, in order to increase the fault tolerance of the system, we replicate the `sKeeper' responsibility in the 2 following nodes in the hashring, so if one of these fails, another is assigned.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{img/chunking.jpg}
  \caption{A file partitioned in several chunks, each with its corresponding hashes that correspond to nodeIds}
  \label{fig:chunking}
\end{figure}

In Figure~\ref{fig:skeepersholder}, we can find the `sKeeper' and `sHolder' relationship. Only the sKeeper performs the chunk hashing and stores the information in the file lookup table, this happens one single time for each chunk, reducing several network hops per message on the consequent searches.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{img/skeepersholder.jpg}
  \caption{Representation of the Node responsible for the file(sKeeper) and it's individual chunk holders(sHolders)}
  \label{fig:skeepersholder}
\end{figure}

Each store file is chunked as soon as it enters the network, this mitigates the risk that would be present if we were transferring files with considerable sizes all at once, starving the network and the node's heap. The only point where the file gets glued together again is when it leaves the network and sent to the user and even this could be made to perform chunk transfer in parallel to the client directly.

Human's Cloud adapts the Load Balancing virtual server's method, by using the same strategy of global load, but by transferring files between sHolders and not an entire virtual server, updating the respective sKeeper accordingly. 

Files are storage as objects in a indexedDB type storage, provided by the leveljs module. 

\subsection{Distributed Job Scheduling}

Job coordination is one of the main challenges in a completely distributed environment, in a sustainable scalable way. Traditionally in the client-server model, we have the possibility to select one of the nodes to be the job coordinator, to implement this in a P2P network, we take advantage of the DHT, to select randomly one of the ascended nodes to be the `jKeeper', the node responsible for coordinating the job in an environment a P2P network.

The `jKeeper', is responsible for contacting the `sKeepers' of each individual file, and coordinate them to command each of `sHolder' to perform the desired computation on the file.

All the steps during the computation are journaled in the Job log, store with the coordinator, and replicated in the 2 following nodes for Fault Tolerance measure.

All the coordination happens in the ascended Node ring, however, in order to take advantage of the normal node ring resources, `sHolders' are allowed to offload the computation to process this job in the `normal hash ring', this is done by sending a probe, asking for `volunteers' for a job, when the threshold required is met, the orchestration starts, where the `sHolder' transfers the data and the assets necessary for processing it.


--------
NOTA:
Estou a actualizar o distributed job scheduling para ter o algoritmo
----
% TODO Fazer aqui um boneco

% note como processar ficheiros que dependem de todos os chunks, not :)


% GET Inspired 

% search_in_index (gridlet) { best_node = local_search (gridlet.specifications); node_utility = best_node.utility;
% best_neighbour = neighbour_search (gridlet.specifications); neighbor_ utility = best_neighbor. utility;
% if(node_ utility < 0 && neighbor_ utility < 0) return fail;
% }
% if(node_ utility >= neighbor_ utility) best_node.execute (gridlet);
% else best_neighbor.search_in_index (gridlet); Figure


\subsection{Reputation Mechanism}

The reputation mechanism present will enable the network to identify the nodes that show more availability and have the necessary means to ascend and take a more important role. In order to evaluate each node, we define several metrics, these are: uptime, number of job completions, network throughput and computational resources(CPU) available, being the uptime, the most important, to assure stability. The reputation metric is calculated as follows:

\begin{itemize}
  \item $reputation = \alpha * log(uptime) + \beta * log(job completions) + \gamma * log(network throughput) + \delta * log (CPU)$ 
  \item $\alpha+ \beta+ \gamma+ \delta = 1$
  \item $\alpha > \beta + \gamma + \delta$
\end{itemize}

We chose to normalize the metrics and give more importance to the uptime of the system, because this is the one enabling a more stable network for storage.

The reputation of each node is stored with its node identifier on the `ascended hash ring', each time a job is completely successfully, his score gets updated and in case it reaches the required level to ascend, the jKeeper that was updating his score will enable and deploy the remaining features(storage and job schedule module) he needed to join the ascended group. 

\subsection{Client API and CLI}

The client API goal is to be familiar to experienced developers using cloud providers today and at the same time, respect the Unix philosophy, where the job work flows are composed by a stream of assets, that do one thing and one thing well. It will support CRUD operations through a REST with JSON API, a filesystem-like interface(directories and objects), where user is identified by a username and it maps to the paths shown in Table~\ref{tbl:dirrepnet}: 

\begin{table}
  \begin{tabular}{ p{3cm} | p{9cm} }
  % \hline 
  Directory & Description \\
  \hline 
  /:username/jobs/ & where new jobs can be inserted by the user \\
  \hline
  /:username/jobs-reports/ & Job status. The user is only able to read and delete the records, they are created by the system. \\
  \hline 
  /:username/home/ &  Private user store, only place where the user has write access \\
  \hline   
  /:username/reports/ & Usage and Access log reports. \\ 
  \hline   
  /nodes/ & Registry of all the nodes in the network with their metadata, the user can only read this folder. \\
  % \hline   
  \end{tabular}
  \caption{Directories representation inside the network}
  \label{tbl:dirrepnet}
\end{table}

We provide a REST API for the developer to use the reason behind this design decision is to create a familiar interface to the majority of webdevelopers. The server replying to this requests defined in Table~\ref{tbl:restapi}, can be a public our private proxy, in the user machine, behind a company firewall or as a public service available to the community, so it remains portable and doesn't lock the user to a provider.

\begin{table}
  \begin{tabular}{ l | c | p{6cm} }
    \multicolumn{3}{ l }{\textbf{Directories}} \\
    Action: PutDirectory & \multicolumn{2}{l}{PUT /:username/home/[:directory]/:directory} \\
    Action: ListDirectory & \multicolumn{2}{l}{GET /:username/home/[:directory]/:directory} \\
    Action: DeleteDirectory & \multicolumn{2}{l}{DELETE /:username/home/[:directory]/:directory} \\
    \hline 
    \multicolumn{3}{ l }{\textbf{Files}} \\
    Action: PutFile & \multicolumn{2}{l}{PUT /:username/home/[:directory]/:filename} \\
    Action: GetFile & \multicolumn{2}{l}{GET /:username/home/[:directory]/:filename} \\
    Action: DeleteFile & \multicolumn{2}{l}{DELETE /:username/home/[:directory]/:filename} \\
    \multicolumn{3}{p{12cm}}{\textbf{note:} PutFile and GetFile, the body of the request and response  respectively is the file} \\
    \hline 
    \multicolumn{3}{ l }{\textbf{Jobs}} \\
    Action: CreateJob & \multicolumn{2}{l}{PUT /:username/jobs} \\
    Action: CancelJob & \multicolumn{2}{l}{DELETE /:username/jobs/:jobId} \\
    Action: ListJobs & \multicolumn{2}{l}{GET /:username/jobs} \\
    Action: GetJob & \multicolumn{2}{l}{GET /:username/home/jobs/:jobId} \\
    Action: GetJobOutput & \multicolumn{2}{l}{GET /:username/jobs-reports/:jobId} \\
    \multicolumn{3}{p{12cm}}{\textbf{note:} Create Job, several arguments are passed, most importantly, an array named ``phases'' that includes the orders with assets necessary in order to execute the job(e.g. `grep -ci' or if its a new asset, it should be a JS object with a closure.)} \\
    \hline 
  \end{tabular}
  \caption{REST API - 1st draft, might change with implementation}
  \label{tbl:restapi}
\end{table}

We are also including a CLI tool to enable quickly bash scripts for computation jobs and file storage, this CLI uses in the background the API defined in Table~\ref{tbl:restapi}. For example, if we are looking for video transcoding: ``hcjob create /path/to/file | ffmpeg | /path/to/out.webm''. The rest of the commands are:
  
\begin{itemize}
  \item \textbf{\$ hcls}  - List files in a directory
  \item \textbf{\$ hcget} - Get an object stored
  \item \textbf{\$ hcput} - Store an object
  \item \textbf{\$ hcjob} - Initialize a job   
\end{itemize} 
